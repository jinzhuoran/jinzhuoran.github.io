<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Zhuoran Jin </title> <meta name="author" content="Zhuoran Jin"> <meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. "> <meta name="keywords" content="jin-zhuoran"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo.png?86871bc3c95e8f0d5907af2bf35778b9"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://jinzhuoran.github.io/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">About <span class="sr-only">(current)</span> </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">Projects &amp; Resources </a> </li> <li class="nav-item "> <a class="nav-link" href="/awards/">Awards &amp; Services </a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title"> Zhuoran Jin </h1> <p class="desc">Fourth-year Ph.D student@Institute of Automation, Chinese Academy of Sciences</p> </header> <article> <div class="profile float-left"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/bio_pic-480.webp 480w,/assets/img/bio_pic-800.webp 800w,/assets/img/bio_pic-1400.webp 1400w," sizes="(min-width: 930px) 270.0px, (min-width: 576px) 30vw, 95vw" type="image/webp"> <img src="/assets/img/bio_pic.jpg?66551af3c5c9c8aaca0a43db510a7c25" class="img-fluid z-depth-1 rounded" width="100%" height="auto" alt="bio_pic.jpg" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> <div class="more-info"> <a href="https://scholar.google.com/citations?user=Am8WsCkAAAAJ&amp;hl=en" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar-square ai-2x"></i></a> <a href="https://github.com/jinzhuoran" rel="external nofollow noopener" target="_blank"><i class="fa-brands fa-square-github fa-2x"></i></a> <a href="https://www.semanticscholar.org/author/Zhuoran-Jin/2152843772" rel="external nofollow noopener" target="_blank"><i class="ai ai-semantic-scholar-square ai-2x"></i></a> </div> </div> <div class="clearfix"> <p>I am a 4th-year Ph.D student at the Natural Language Processing and Knowledge Engineering Group (NLPKE), <a href="http://www.ia.ac.cn/" rel="external nofollow noopener" target="_blank">Institute of Automation, Chinese Academy of Sciences (CASIA)</a>. I am fortunate to be advised by <a href="https://nlpr-web.ia.ac.cn/cip/english/~junzhao/index.html" rel="external nofollow noopener" target="_blank">Prof. Jun Zhao</a>. Before that, I obtained my B.E. degree in Software Engineering from <a href="https://www.neu.edu.cn/" rel="external nofollow noopener" target="_blank">Northeastern University (NEU)</a> in 2021. My research interests include <strong>natural language processing</strong>, <strong>large language models</strong>, and <strong>knowledge engineering</strong>.</p> <p>My research is dedicated to bridging the gap between <strong>large language models</strong> and <strong>human knowledge frameworks</strong>, with a focus on <strong>expanding knowledge boundaries</strong>, <strong>erasing harmful knowledge</strong>, and <strong>improving reasoning capabilities</strong>. Recently, I have also become increasingly interested in <strong>multimodal large language models</strong>, particularly in understanding and enhancing their ability to <strong>automatically solve complex and meaningful real-world problems</strong>. My primary research areas are:</p> <ul> <li> <strong>Retrieval-Augmented Generation</strong>: RAG can effectively expand the internal memory boundaries of LLMs by providing external context. My work focuses on: (1) leveraging feedback reward signals from LLMs to improve retrieval quality (<a href="https://aclanthology.org/2023.findings-emnlp.443.pdf" rel="external nofollow noopener" target="_blank">InstructoR, EMNLP 2023</a>); (2) investigating the mechanisms underlying knowledge conflicts between internal memory and external context (<a href="https://aclanthology.org/2024.lrec-main.1466.pdf" rel="external nofollow noopener" target="_blank">Tug-of-War Between Knowledge, COLING 2024</a> and <a href="https://aclanthology.org/2024.findings-acl.70.pdf" rel="external nofollow noopener" target="_blank">Cutting Off the Head Ends the Conflict, ACL 2024</a>); and (3) aligning RAG model behavior with human preferences through reward modeling (<a href="https://arxiv.org/pdf/2412.13746" rel="external nofollow noopener" target="_blank">RAG-RewardBench, ACL 2025</a>).</li> <li> <strong>Machine Unlearning</strong>: Machine unlearning enables the targeted removal of sensitive, harmful, or copyrighted knowledge from models. To better evaluate unlearning in the domain of LLMs, we propose a Real-World Knowledge Unlearning benchmark (<a href="https://openreview.net/forum?id=wOmtZ5FgMH" rel="external nofollow noopener" target="_blank">RWKU, NeurIPS 2024</a>). Building on this, we reveal the vulnerability of existing unlearning algorithms to adversarial attacks and propose Latent Adversarial Unlearning for robust unlearning (<a href="https://ojs.aaai.org/index.php/AAAI/article/view/34769" rel="external nofollow noopener" target="_blank">LAU, AAAI 2025</a>). Moreover, to improve the naturalness of model responses after unlearning, we introduce an on-policy reinforcement learning framework that performs refusal boundary optimization (<a href="https://github.com/chenlong-clock/RULE-Unlearn" rel="external nofollow noopener" target="_blank">RULE</a>).</li> <li> <strong>Reward Modeling</strong>: Reward models serve as a critical proxy for human values, guiding optimization in RLHF. We explore reward modeling in the contexts of RAG (<a href="https://arxiv.org/pdf/2412.13746" rel="external nofollow noopener" target="_blank">RAG-RewardBench, ACL 2025</a>), agent (Agent-RewardBench, ACL 2025), and omni-modal scenarios (<a href="https://omnireward.github.io/" rel="external nofollow noopener" target="_blank">Omni-Reward</a>).</li> <li> <strong>Multimodal Reasoning</strong>: Multimodal reasoning is a core capability for AI systems to solve real-world tasks. However, the extent to which current models have truly advanced in this ability remains unclear. To address this gap, my research mainly involves: (1) exploring what multimodal CoT reasoning can and cannot do, and revealing the reasoning limitation known as “Look Shallow, Think Deep” (<a href="https://openreview.net/forum?id=wOmtZ5FgMH" rel="external nofollow noopener" target="_blank">Look Shallow, Think Deep</a>); (2) proposing a benchmark for multimodal video reasoning that exposes the challenges current models face in conducting long-range, multi-frame inference (<a href="https://openreview.net/forum?id=wOmtZ5FgMH" rel="external nofollow noopener" target="_blank">MMR-V</a>).</li> </ul> <p>If you are interested in my work or want to collaborate, feel free to contact me via: zhuoran.jin[at]nlpr[dot]ia[dot]ac.cn.</p> </div> <h2> <a href="/news/" style="color: inherit">News</a> </h2> <div class="news"> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 12%">May 26, 2025</th> <td> Three papers are released on arXiv, exploring omni-modal reward modeling, demystifying multimodal CoT reasoning, and benchmarking multimodal video reasoning. </td> </tr> <tr> <th scope="row" style="width: 12%">May 16, 2025</th> <td> Five paper are accepted by ACL 2025. </td> </tr> </table> </div> </div> <h2> <a href="/publications/" style="color: inherit">Selected Publications</a> </h2> <div class="publications"> <ol class="bibliography"> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmcot-480.webp 480w,/assets/img/publication_preview/mmcot-800.webp 800w,/assets/img/publication_preview/mmcot-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/mmcot.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="mmcot.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:journals/corr/abs-2412-13746" class="col-sm-9"> <div class="title">Look Shallow, Think Deep: What Multimodal Chain-of-Thought Reasoning Can and Cannot Do</div> <div class="author"> <em>Zhuoran Jin<sup>*</sup></em>, Kejian Zhu<sup>*</sup>, Hongbang Yuan, Yupu Hao, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao </div> <div class="periodical"> <em>arXiv preprint</em> (<b>arXiv</b>), 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2412-13746</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Zhuoran and Zhu, Kejian and Yuan, Hongbang and Hao, Yupu and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Look Shallow, Think Deep: What Multimodal Chain-of-Thought Reasoning Can and Cannot Do}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/omni-480.webp 480w,/assets/img/publication_preview/omni-800.webp 800w,/assets/img/publication_preview/omni-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/omni.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="omni.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:journals/corr/abs-2412-13747" class="col-sm-9"> <div class="title">Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences</div> <div class="author"> <em>Zhuoran Jin<sup>*</sup></em>, Hongbang Yuan<sup>*</sup>, Kejian Zhu<sup>*</sup>, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao </div> <div class="periodical"> <em>arXiv preprint</em> (<b>arXiv</b>), 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/collections/jinzhuoran/omnireward-682446d99aeab6db78d77af5" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2412-13747</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Zhuoran and Yuan, Hongbang and Zhu, Kejian and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Omni-Reward: Towards Generalist Omni-Modal Reward Modeling with Free-Form Preferences}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/mmrv-480.webp 480w,/assets/img/publication_preview/mmrv-800.webp 800w,/assets/img/publication_preview/mmrv-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/mmrv.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="mmrv.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:journals/corr/abs-2412-13748" class="col-sm-9"> <div class="title">MMR-V: What’s Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos</div> <div class="author"> Kejian Zhu, <em>Zhuoran Jin</em>, Hongbang Yuan, Jiachun Li, Shangqing Tu, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao </div> <div class="periodical"> <em>arXiv preprint</em> (<b>arXiv</b>), 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/datasets/JokerJan/MMR-VBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2412-13748</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhu, Kejian and Jin, Zhuoran and Yuan, Hongbang and Li, Jiachun and Tu, Shangqing and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{MMR-V: What's Left Unsaid? A Benchmark for Multimodal Deep Reasoning in Videos}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">arXiv</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rule-480.webp 480w,/assets/img/publication_preview/rule-800.webp 800w,/assets/img/publication_preview/rule-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/rule.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="rule.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:journals/corr/abs-2412-13749" class="col-sm-9"> <div class="title">RULE: Reinforcement UnLEarning Achieves Forget-retain Pareto Optimality</div> <div class="author"> Chenlong Zhang, <em>Zhuoran Jin</em>, Hongbang Yuan, Jiaheng Wei, Tong Zhou, Kang Liu, Jun Zhao, and Yubo Chen </div> <div class="periodical"> <em>arXiv preprint</em> (<b>arXiv</b>), 2025 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://github.com/chenlong-clock/RULE-Unlearn" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@article</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2412-13749</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Zhang, Chenlong and Jin, Zhuoran and Yuan, Hongbang and Wei, Jiaheng and Zhou, Tong and Liu, Kang and Zhao, Jun and Chen, Yubo}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RULE: Reinforcement UnLEarning Achieves Forget-retain Pareto Optimality}</span><span class="p">,</span>
  <span class="na">journal</span> <span class="p">=</span> <span class="s">{arXiv preprint}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">ACL Findings</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ragreward-480.webp 480w,/assets/img/publication_preview/ragreward-800.webp 800w,/assets/img/publication_preview/ragreward-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/ragreward.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="ragreward.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:journals/corr/abs-2412-13754" class="col-sm-9"> <div class="title">RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment</div> <div class="author"> <em>Zhuoran Jin</em>, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao </div> <div class="periodical"> In <em>Annual Meeting of the Association for Computational Linguistics</em> (<b>ACL Findings</b>) , 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2412.13746" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:journals/corr/abs-2412-13754</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Zhuoran and Yuan, Hongbang and Men, Tianyi and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented Generation for Preference Alignment}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">AAAI</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/lau-480.webp 480w,/assets/img/publication_preview/lau-800.webp 800w,/assets/img/publication_preview/lau-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/lau.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="lau.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:conf/aaai/YuanJC0LZ25" class="col-sm-9"> <div class="title">Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models</div> <div class="author"> Hongbang Yuan<sup>*</sup>, <em>Zhuoran Jin<sup>*</sup></em>, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao </div> <div class="periodical"> In <em>Annual AAAI Conference on Artificial Intelligence</em> (<b>AAAI</b>) , 2025 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2408.10682" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://ojs.aaai.org/index.php/AAAI/inproceedings/view/34769" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/HongbangYuan/RobustUnlearning" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/aaai/YuanJC0LZ25</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Yuan, Hongbang and Jin, Zhuoran and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Walsh, Toby and Shah, Julie and Kolter, Zico}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual AAAI Conference on Artificial Intelligence}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{25769--25777}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{{AAAI} Press}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2025}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50" style="background-color:#00369f"> <div>NeurIPS</div> </abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/rwku-480.webp 480w,/assets/img/publication_preview/rwku-800.webp 800w,/assets/img/publication_preview/rwku-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/rwku.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="rwku.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:conf/nips/JinCWHYL00024" class="col-sm-9"> <div class="title">RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models</div> <div class="author"> <em>Zhuoran Jin</em>, Pengfei Cao, Chenhao Wang, Zhitao He, Hongbang Yuan, Jiachun Li, Yubo Chen, Kang Liu, and Jun Zhao </div> <div class="periodical"> In <em>Annual Conference on Neural Information Processing Systems</em> (<b>NeurIPS</b>) , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2406.10890" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="http://papers.nips.cc/paper_files/paper/2024/hash/b1f78dfc9ca0156498241012aec4efa0-Abstract-Datasets_and_Benchmarks_Track.html" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://github.com/jinzhuoran/RWKU" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a> <a href="https://rwku-bench.github.io/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/nips/JinCWHYL00024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Zhuoran and Cao, Pengfei and Wang, Chenhao and He, Zhitao and Yuan, Hongbang and Li, Jiachun and Chen, Yubo and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Globersons, Amir and Mackey, Lester and Belgrave, Danielle and Fan, Angela and Paquet, Ulrich and Tomczak, Jakub M. and Zhang, Cheng}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Conference on Neural Information Processing Systems}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">ACL Findings</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/ph3-480.webp 480w,/assets/img/publication_preview/ph3-800.webp 800w,/assets/img/publication_preview/ph3-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/ph3.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="ph3.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:conf/acl/JinCY0XLJ0024" class="col-sm-9"> <div class="title">Cutting Off the Head Ends the Conflict: A Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models</div> <div class="author"> <em>Zhuoran Jin</em>, Pengfei Cao, Hongbang Yuan, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, and Jun Zhao </div> <div class="periodical"> In <em>Annual Meeting of the Association for Computational Linguistics</em> (<b>ACL Findings</b>) , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2402.18154" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.findings-acl.70" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/acl/JinCY0XLJ0024</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Zhuoran and Cao, Pengfei and Yuan, Hongbang and Chen, Yubo and Xu, Jiexin and Li, Huaijun and Jiang, Xiaojian and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Ku, Lun{-}Wei and Martins, Andre and Srikumar, Vivek}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Cutting Off the Head Ends the Conflict: {A} Mechanism for Interpreting and Mitigating Knowledge Conflicts in Language Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Annual Meeting of the Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{1193--1215}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">COLING</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/tug-480.webp 480w,/assets/img/publication_preview/tug-800.webp 800w,/assets/img/publication_preview/tug-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/tug.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="tug.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:conf/coling/JinC0LJXLZ24" class="col-sm-9"> <div class="title">Tug-of-War between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models</div> <div class="author"> <em>Zhuoran Jin</em>, Pengfei Cao, Yubo Chen, Kang Liu, Xiaojian Jiang, Jiexin Xu, Qiuxia Li, and Jun Zhao </div> <div class="periodical"> In <em>International Conference on Computational Linguistics</em> (<b>COLING</b>) , 2024 </div> <div class="periodical"> </div> <div class="links"> <a href="http://arxiv.org/abs/2402.14409" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2024.lrec-main.1466" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/coling/JinC0LJXLZ24</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Zhuoran and Cao, Pengfei and Chen, Yubo and Liu, Kang and Jiang, Xiaojian and Xu, Jiexin and Li, Qiuxia and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Calzolari, Nicoletta and Kan, Min{-}Yen and Hoste, V{\'{e}}ronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{Tug-of-War between Knowledge: Exploring and Resolving Knowledge Conflicts in Retrieval-Augmented Language Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{International Conference on Computational Linguistics}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{16867--16878}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2024}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col col-sm-3 abbr"> <abbr class="badge w-50">EMNLP Findings</abbr> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/publication_preview/instructor-480.webp 480w,/assets/img/publication_preview/instructor-800.webp 800w,/assets/img/publication_preview/instructor-1400.webp 1400w," sizes="95vw" type="image/webp"> <img src="/assets/img/publication_preview/instructor.png" class="preview z-depth-1 rounded" width="180px" height="120px" alt="instructor.png" data-zoomable loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </source></picture> </figure> </div> <div id="DBLP:conf/emnlp/JinC0LZ23" class="col-sm-9"> <div class="title">InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models</div> <div class="author"> <em>Zhuoran Jin</em>, Pengfei Cao, Yubo Chen, Kang Liu, and Jun Zhao </div> <div class="periodical"> In <em>Conference on Empirical Methods in Natural Language Processing</em> (<b>EMNLP Findings</b>) , 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2023.findings-emnlp.443/" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">DBLP:conf/emnlp/JinC0LZ23</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Jin, Zhuoran and Cao, Pengfei and Chen, Yubo and Liu, Kang and Zhao, Jun}</span><span class="p">,</span>
  <span class="na">editor</span> <span class="p">=</span> <span class="s">{Bouamor, Houda and Pino, Juan and Bali, Kalika}</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{InstructoR: Instructing Unsupervised Conversational Dense Retrieval with Large Language Models}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Conference on Empirical Methods in Natural Language Processing}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{6649--6675}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> </ol> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Zhuoran Jin. Last updated: May 27, 2025. <img src="https://badges.toozhao.com/badges/01J3HPVQESCZP39CKV4YD5EGMV/blue.svg"> </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>